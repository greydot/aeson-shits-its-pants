#+TITLE: Aeson ridiculous memory consumption showcase
#+STARTUP: inlineimages

* What is going on here?!

This is a small showcase of how Haskell and the aeson library suffer from very
unfunny case of being probably even worse than Java. And we all know how Java
sucks :(

The example here consists solely of parsing a 14 megabyte randomly generated
piece of JSON into Haskell's ~Data.Map~. One may expect the consumed memory amount
to be on the order of data being processed. But if you run this code, the result may
surprise you.

#+begin_src
$ ./run.sh
999444
        Command being timed: "./aeson-shits-its-pants"
        User time (seconds): 3.23
        System time (seconds): 0.15
        Percent of CPU this job got: 99%
        Elapsed (wall clock) time (h:mm:ss or m:ss): 0:03.40
        Average shared text size (kbytes): 0
        Average unshared data size (kbytes): 0
        Average stack size (kbytes): 0
        Average total size (kbytes): 0
>>>>>>> Maximum resident set size (kbytes): 559976
        Average resident set size (kbytes): 0
        Major (requiring I/O) page faults: 295
        Minor (reclaiming a frame) page faults: 139221
        Voluntary context switches: 340
        Involuntary context switches: 12
        Swaps: 0
        File system inputs: 39078
        File system outputs: 0
        Socket messages sent: 0
        Socket messages received: 0
        Signals delivered: 0
        Page size (bytes): 4096
        Exit status: 0
#+end_src

In effect, the ration of required memory to the size of processed data here is
40 (!!!) to 1.

Granted, not all blame lies on aeson itself. Haskell is terribly inefficient when
it comes to runtime data structures. If you aren't familiar with this, check out
[[https://wiki.haskell.org/GHC/Memory_Footprint][the wiki page on Haskell memory footprint]]. In particular, for this rather contrived
example the ~Data.Map~ would occupy ~6*8*N + N * 2 * (10 + 16)~ or in case or N=1000000,
around 100 megabytes. The rest can be attributed to aeson, minus Haskell's runtime
costs. Consuming over 400 megabytes to parse a 14 megabyte piece of text is still
not very funny :(

* I'm depressed now. What do I do?

Probably avoid using JSON for storing and/or transmitting data. Find some other
serialisation format or even invent something new. I hear, s-exp is still good.

* Your data is wrong!

Yeah, it probably is. There are ways to optimise my code here. Use ~ShortText~ or
even ~ByteString~ instead of ~Text~. But in my defense, you can see code like this
in almost any Haskell web application backend or any other piece of code that
works with JSON. Hopefully, you see my point here. If the default and the most
popular way to write code leads to this result, we definitely have a problem.

* Credits

I was listening to the latest Noise Unit album while writing this, which I greatly
recommend.

Also, drink Bepis. It rocks!

[[./bepis.gif]]
